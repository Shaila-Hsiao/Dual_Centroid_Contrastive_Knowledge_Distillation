{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d72665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import builtins\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import pcl.loader\n",
    "import pcl.builder\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c10b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating teacher model\n",
      "MoCo(\n",
      "  (encoder_q): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  )\n",
      "  (encoder_k): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Teacher model device: cuda:0\n",
      "=> creating student model \n",
      "student model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 初始化老師模型\n",
    "print(\"=> creating teacher model\")\n",
    "teacher_model = pcl.builder.MoCo(\n",
    "    models.__dict__['resnet50'],\n",
    "    dim=128, r=512, m=0.9, T=0.7, mlp=False)\n",
    "# 替換分類頭\n",
    "# teacher_model.fc = nn.Linear(128, 200).cuda()\n",
    "teacher_model = teacher_model.cuda(0)\n",
    "print(teacher_model)\n",
    "print(f\"Teacher model device: {next(teacher_model.parameters()).device}\")\n",
    "\n",
    "print(\"=> creating student model \")\n",
    "model = pcl.builder.MoCo(\n",
    "    models.__dict__['resnet50'],\n",
    "    dim=128, r=512, m=0.9, T=0.7, mlp=False)\n",
    "model = model.cuda(0)\n",
    "print(f\"student model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4c58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda(0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.005,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=1e-4)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c3c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dir path : C:\\Users\\k3866\\Documents\\Datasets\\tiny_imagenet\\tiny-imagenet-200\\train\n",
      "val dir path : C:\\Users\\k3866\\Documents\\Datasets\\tiny_imagenet\\tiny-imagenet-200\\val\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\k3866\\Documents\\Datasets\\tiny_imagenet\\tiny-imagenet-200\"\n",
    "traindir = os.path.join(path, 'train')\n",
    "valdir = os.path.join(path, 'val')\n",
    "print(\"train dir path :\",traindir)\n",
    "print(\"val dir path :\",valdir)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8eb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = [\n",
    "    transforms.RandomResizedCrop(64, scale=(0.2, 1.)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([pcl.loader.GaussianBlur([.1, 2.])], p=0.5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51ce6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_augmentation = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fdc56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pcl.loader.ImageFolderInstance(\n",
    "    traindir,\n",
    "    pcl.loader.TwoCropsTransform(transforms.Compose(augmentation)))\n",
    "    \n",
    "eval_dataset = pcl.loader.ImageFolderInstance(\n",
    "    traindir,\n",
    "    eval_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6e1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=True,\n",
    "    num_workers=16, pin_memory=True, drop_last=True)\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_dataset, batch_size=256, shuffle=False,\n",
    "    num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f3a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['queue', 'queue_ptr']\n",
      "Unexpected keys: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MoCo(\n",
       "  (encoder_q): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  )\n",
       "  (encoder_k): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 加載預訓練的老師模型權重\n",
    "# checkpoint_path = \"checkpoint.pth.tar\"\n",
    "checkpoint_path = r\"C:\\Users\\k3866\\Documents\\PretrianedModel\\Moco\\checkpoint_0099.pth.tar\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\",weights_only=True)\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "# 處理權重名稱\n",
    "new_state_dict = {}\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(\"encoder_q\"):\n",
    "        new_state_dict[f\"encoder_q.{k[len('encoder_q.'):]}\"] = state_dict[k]\n",
    "    elif k.startswith(\"encoder_k\"):\n",
    "        new_state_dict[f\"encoder_k.{k[len('encoder_k.'):]}\"] = state_dict[k]\n",
    "\n",
    "# 加載權重到老師模型\n",
    "msg = teacher_model.load_state_dict(new_state_dict, strict=False)\n",
    "print(f\"Missing keys: {msg.missing_keys}\")\n",
    "print(f\"Unexpected keys: {msg.unexpected_keys}\")\n",
    "teacher_model = teacher_model.cuda(0)\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6353479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_centroids(teacher_model, train_loader, num_classes, feature_dim):\n",
    "    print('Computing Centroid via SSLCon model...')\n",
    "    centroids = torch.zeros(num_classes, feature_dim).cuda()\n",
    "    print(\"centroid :\",centroids.shape)\n",
    "    print(centroids)\n",
    "    counts = torch.zeros(num_classes).cuda()\n",
    "    print(\"counts :\",counts.shape)\n",
    "    print(counts)\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(train_loader, desc=\"Computing Centroids\")\n",
    "        for idx, (images, labels) in enumerate(progress_bar):\n",
    "            progress_bar.set_description(f\"[{idx + 1}/{len(train_loader)}]\")\n",
    "            image = images[0].cuda()\n",
    "            labels = labels.cuda()\n",
    "            features = teacher_model.encoder_q(image)\n",
    "            for i in range(num_classes):\n",
    "                mask = labels == i\n",
    "                # print(f\"Mask (labels == {i}): {mask}\")  # 打印該類別的 mask，表示哪些樣本屬於該類別\n",
    "                centroids[i] += features[mask].sum(dim=0)\n",
    "                # print(f\"Updated centroids[{i}]: {centroids[i]}\")  # 打印更新後的 centroids[i]\n",
    "                counts[i] += mask.sum()\n",
    "                # print(f\"Updated counts[{i}]: {counts[i]}\")  # 打印更新後的 counts[i]\n",
    "    centroids /= counts.unsqueeze(1)\n",
    "    # centroids = F.normalize(centroids, dim=-1)\n",
    "    return centroids, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d84a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: torch.Size([256, 128])\n",
      "Computing Centroid via SSLCon model...\n",
      "centroid : torch.Size([200, 128])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "counts : torch.Size([200])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[390/390]: 100%|██████████| 390/390 [01:49<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class centroids computed. Centroids shape: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    for images, _ in train_loader:\n",
    "        features = teacher_model.encoder_k(images[0].cuda())\n",
    "        feature_dim = features.shape[1]\n",
    "        print(f\"Feature shape: {features.shape}\")\n",
    "        break\n",
    "num_classes = len(train_loader.dataset.classes)\n",
    "class_centroids, class_counts = set_centroids(teacher_model, train_loader, num_classes, feature_dim)\n",
    "print(f\"Class centroids computed. Centroids shape: {class_centroids.shape}\")\n",
    "# print(class_centroids)\n",
    "# class_centroids = get_class_centroids(teacher_model, train_loader,gpu)\n",
    "class_centroids = class_centroids.cuda(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67007bec",
   "metadata": {},
   "source": [
    "### kmeans 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541df728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(z_q, class_centroids, eps=1e-8):\n",
    "    # 正規化\n",
    "    z_q_norm = F.normalize(z_q, p=2, dim=1,eps=eps)\n",
    "    class_centroids_norm = F.normalize(class_centroids, p=2, dim=1,eps=eps)\n",
    "    # 矩陣乘法計算餘弦相似度\n",
    "    return torch.matmul(z_q_norm, class_centroids_norm.t())\n",
    "\n",
    "def compute_features(eval_loader, model):\n",
    "    print('Computing features...')\n",
    "    model.eval()\n",
    "    features = torch.zeros(len(eval_loader.dataset), 128).cuda()\n",
    "    for i, (images, index) in enumerate(tqdm(eval_loader)):\n",
    "        with torch.no_grad():\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            feat = model(images, is_eval=True)\n",
    "            features[index] = feat\n",
    "    return features.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d08a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_kmeans(x):\n",
    "#     print('performing kmeans clustering')\n",
    "#     results = {'im2cluster': [], 'centroids': [], 'density': []}\n",
    "#     num_cluster = \"1000,1500,2000\"\n",
    "#     num_cluster = num_cluster.split(',')\n",
    "#     for seed, num_cluster in enumerate(num_cluster):\n",
    "#         d = x.shape[1]\n",
    "#         k = int(num_cluster)\n",
    "#         clus = faiss.Clustering(d, k)\n",
    "#         clus.verbose = True\n",
    "#         clus.niter = 20\n",
    "#         clus.nredo = 5\n",
    "#         clus.seed = seed\n",
    "#         clus.max_points_per_centroid = 200\n",
    "#         clus.min_points_per_centroid = 10\n",
    "\n",
    "#         res = faiss.StandardGpuResources()\n",
    "#         cfg = faiss.GpuIndexFlatConfig()\n",
    "#         cfg.useFloat16 = False\n",
    "#         cfg.device = 0\n",
    "#         index = faiss.GpuIndexFlatL2(res, d, cfg)\n",
    "\n",
    "#         clus.train(x, index)\n",
    "\n",
    "#         D, I = index.search(x, 1)\n",
    "#         im2cluster = [int(n[0]) for n in I]\n",
    "\n",
    "#         centroids = faiss.vector_to_array(clus.centroids).reshape(k, d)\n",
    "\n",
    "#         Dcluster = [[] for c in range(k)]\n",
    "#         for im, i in enumerate(im2cluster):\n",
    "#             Dcluster[i].append(D[im][0])\n",
    "\n",
    "#         density = np.zeros(k)\n",
    "#         for i, dist in enumerate(Dcluster):\n",
    "#             if len(dist) > 1:\n",
    "#                 d = (np.asarray(dist) ** 0.5).mean() / np.log(len(dist) + 10)\n",
    "#                 density[i] = d\n",
    "\n",
    "#         dmax = density.max()\n",
    "#         for i, dist in enumerate(Dcluster):\n",
    "#             if len(dist) <= 1:\n",
    "#                 density[i] = dmax\n",
    "\n",
    "#         density = density.clip(np.percentile(density, 10), np.percentile(density, 90))\n",
    "#         density = 0.2 * density / density.mean()\n",
    "\n",
    "#         centroids = torch.Tensor(centroids).cuda()\n",
    "#         centroids = nn.functional.normalize(centroids, p=2, dim=1)\n",
    "\n",
    "#         im2cluster = torch.LongTensor(im2cluster).cuda()\n",
    "#         density = torch.Tensor(density).cuda()\n",
    "\n",
    "#         results['centroids'].append(centroids)\n",
    "#         results['density'].append(density)\n",
    "#         results['im2cluster'].append(im2cluster)\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00e64a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_masking(features, cluster_assignments):\n",
    "    \"\"\"\n",
    "    根據 clustering assignments 和遮罩策略對特徵數據應用遮罩。\n",
    "    - features: 特徵數據，形狀為 (N, D)，N 是樣本數，D 是特徵維度。\n",
    "    - cluster_assignments: 每個樣本的分群結果。\n",
    "    - args: 包含遮罩模式和相關參數的配置。\n",
    "\n",
    "    返回:\n",
    "    - masked_features: 經過遮罩的特徵數據。\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    import scipy.spatial as ss\n",
    "    import numpy as np\n",
    "\n",
    "    # 初始化變數\n",
    "    clusters = defaultdict(list)  # 每個 cluster 包含的樣本索引\n",
    "    max_dis_list = []  # 儲存需要遮罩的樣本索引\n",
    "    mask_mode = 'mask_farthest'\n",
    "    dist_threshold= 0.3\n",
    "    proportion = 0.1\n",
    "    # 將數據根據 cluster_assignments 分組\n",
    "    for idx, cluster_id in enumerate(cluster_assignments):\n",
    "        clusters[cluster_id].append(idx)\n",
    "    print(f\"Number of clusters: {len(clusters)}\")\n",
    "\n",
    "    # 遍歷每個 cluster，根據遮罩策略篩選需要遮罩的樣本\n",
    "    for cluster_id, indices in clusters.items():\n",
    "        cluster_features = features[indices]  # 提取該 cluster 的特徵\n",
    "        centroid = np.mean(cluster_features, axis=0)  # 計算該 cluster 的質心\n",
    "        print(f\"Cluster {cluster_id}: Centroid computed.\")\n",
    "        # print(f\"Centroid (first 5 values): {centroid[:5]}\")\n",
    "\n",
    "        # 計算每個樣本與質心的歐氏距離\n",
    "        distances = [ss.distance.euclidean(centroid, features[idx]) for idx in indices]\n",
    "        print(f\"Cluster {cluster_id}: Computed distances (first 5): {distances[:5]}\")\n",
    "\n",
    "        if mask_mode == 'mask_farthest':\n",
    "            # 遮罩距離質心最遠的樣本\n",
    "            max_idx = indices[np.argmax(distances)]\n",
    "            max_dis_list.append(max_idx)\n",
    "            print(f\"Cluster {cluster_id}: Masking farthest sample (index {max_idx}).\")\n",
    "        elif mask_mode == 'mask_threshold':\n",
    "            # 遮罩距離超過指定閾值的樣本\n",
    "            for idx, dist in zip(indices, distances):\n",
    "                if dist > dist_threshold:\n",
    "                    max_dis_list.append(idx)\n",
    "                    print(f\"Cluster {cluster_id}: Masking sample (index {idx}) with distance {dist:.3f}.\")\n",
    "        elif mask_mode == 'mask_proportion':\n",
    "            # 遮罩指定比例的最遠樣本\n",
    "            num_to_mask = int(len(indices) * proportion)\n",
    "            sorted_indices = sorted(zip(indices, distances), key=lambda x: x[1], reverse=True)\n",
    "            max_dis_list.extend([x[0] for x in sorted_indices[:num_to_mask]])\n",
    "            print(f\"Cluster {cluster_id}: Masking {num_to_mask} farthest samples.\")\n",
    "    # 將被遮罩的樣本設為零向量\n",
    "    masked_features = features.copy()\n",
    "    print(f\"Total masked samples: {len(max_dis_list)}\")\n",
    "    \n",
    "    for idx in max_dis_list:\n",
    "        print(f\"Masking sample at index {idx}.\")\n",
    "        masked_features[idx] = 0.0\n",
    "    print(\"Max dis list len : \",len(max_dis_list))\n",
    "    # print(\"Max features:\",masked_features)\n",
    "    print(\"Shape : \",masked_features.shape)\n",
    "    return masked_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d57e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(features):\n",
    "    \"\"\"\n",
    "    執行 KMeans 聚類，並應用遮罩策略。\n",
    "    - features: 特徵數據，形狀為 (N, D)。\n",
    "    - args: 包含 KMeans 和遮罩相關配置的參數。\n",
    "\n",
    "    返回:\n",
    "    - results: 包含質心、密度和分群結果的字典。\n",
    "    \"\"\"\n",
    "    print('Performing kmeans clustering with masking...')\n",
    "    results = {'im2cluster': [], 'centroids': [], 'density': []}\n",
    "    masked_features = features.clone()  # 初始化 masked_features\n",
    "    num_cluster = \"1000,1500,2000\"\n",
    "    num_cluster = num_cluster.split(',')\n",
    "    mask_mode = 'mask_farthest'\n",
    "    # dist_threshold= 0.3\n",
    "    # proportion = 0.1\n",
    "    for seed, num_cluster in enumerate(num_cluster):\n",
    "    # for seed, num_cluster in enumerate(args.num_cluster):\n",
    "        d = masked_features.shape[1]\n",
    "        k = int(num_cluster)\n",
    "        \n",
    "        # 初始化 FAISS 聚類\n",
    "        clus = faiss.Clustering(d, k)\n",
    "        clus.verbose = True\n",
    "        clus.niter = 20\n",
    "        clus.nredo = 5\n",
    "        clus.seed = seed\n",
    "        clus.max_points_per_centroid = 200\n",
    "        clus.min_points_per_centroid = 10\n",
    "\n",
    "        res = faiss.StandardGpuResources()\n",
    "        cfg = faiss.GpuIndexFlatConfig()\n",
    "        cfg.useFloat16 = False\n",
    "        cfg.device = 0\n",
    "        index = faiss.GpuIndexFlatL2(res, d, cfg)\n",
    "\n",
    "        # 執行聚類\n",
    "        clus.train(masked_features.cpu().numpy(), index)\n",
    "\n",
    "        # 搜索最近的聚類中心\n",
    "        D, I = index.search(masked_features.cpu().numpy(), 1)\n",
    "        im2cluster = [int(n[0]) for n in I]\n",
    "\n",
    "        # 計算每個 cluster 的距離\n",
    "        Dcluster = [[] for c in range(k)]\n",
    "        for im, i in enumerate(im2cluster):\n",
    "            Dcluster[i].append(D[im][0])\n",
    "\n",
    "        density = np.zeros(k)\n",
    "        for i, dist in enumerate(Dcluster):\n",
    "            if len(dist) > 1:\n",
    "                density_value = (np.asarray(dist) ** 0.5).mean() / np.log(len(dist) + 10)\n",
    "                density[i] = density_value\n",
    "\n",
    "        dmax = density.max()\n",
    "        for i, dist in enumerate(Dcluster):\n",
    "            if len(dist) <= 1:\n",
    "                density[i] = dmax\n",
    "\n",
    "        density = density.clip(np.percentile(density, 10), np.percentile(density, 90))\n",
    "        density = 0.2 * density / density.mean()\n",
    "\n",
    "        # 添加遮罩檢查之前\n",
    "        # print(f\"Cluster assignments (first 10): {im2cluster[:10]}\")  # 確認分群結果\n",
    "        print(f\"Applying masking with mode: {mask_mode}\")  # 確認遮罩模式\n",
    "\n",
    "        # 應用遮罩策略，更新 masked_features\n",
    "        masked_features = apply_masking(\n",
    "            masked_features.cpu().numpy(),\n",
    "            cluster_assignments=im2cluster,\n",
    "        )\n",
    "        masked_features = torch.tensor(masked_features).cuda()  # 轉換為 PyTorch 張量\n",
    "        # print(\"轉換為 PyTorch 張量的 masked_features\",masked_features,\"shape : \",masked_features.shape)\n",
    "        # print(f\"Masked features (first row): {masked_features[0].cpu().numpy()}\")  # 確認遮罩效果\n",
    "        \n",
    "        # 更新聚類結果\n",
    "        print(\"Cluster K X D\",k,d )\n",
    "        centroids = faiss.vector_to_array(clus.centroids).reshape(k, d)\n",
    "        centroids = torch.tensor(centroids).cuda()\n",
    "        centroids = nn.functional.normalize(centroids, p=2, dim=1)\n",
    "        \n",
    "        im2cluster = torch.LongTensor(im2cluster).cuda()\n",
    "        density = torch.Tensor(density).cuda()\n",
    "\n",
    "        results['density'].append(density)\n",
    "        results['centroids'].append(centroids)\n",
    "        results['im2cluster'].append(im2cluster)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2643c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = 0.005\n",
    "    # if args.cos:\n",
    "    lr *= 0.5 * (1. + math.cos(math.pi * epoch / 200))\n",
    "    # else:\n",
    "    #     for milestone in args.schedule:\n",
    "    #         lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        # print(\"output shape:\",output.shape)\n",
    "        if output is None or target is None:\n",
    "            raise ValueError(\"Output or target is None\")\n",
    "        # print(f\"Output shape: {output.shape}, Target shape: {target.shape}\")\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05780e61",
   "metadata": {},
   "source": [
    "### Process 紀錄的class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f4a3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11e909e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss(teacher_probs, student_probs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    計算 KD (Knowledge Distillation) 損失\n",
    "    - teacher_probs: Teacher 模型的概率分布 (經過 softmax)。\n",
    "    - student_probs: Student 模型的概率分布 (經過 softmax)。\n",
    "    - temperature: 蒸餾溫度，默認為 1.0。較高的溫度可以使概率分布更加平滑。\n",
    "    \"\"\"\n",
    "    # 調整 Teacher 和 Student 的概率分布，加入溫度系數\n",
    "    teacher_probs = F.softmax(teacher_probs / temperature, dim=1)\n",
    "    student_probs = F.softmax(student_probs / temperature, dim=1)\n",
    "    \n",
    "    # 計算 KL 散度損失 (使用 PyTorch 的 kl_div 函數)\n",
    "    loss = F.kl_div(\n",
    "        input=torch.log(student_probs),  # Student 的對數概率分布\n",
    "        target=teacher_probs,           # Teacher 的目標概率分布\n",
    "        reduction='batchmean'           # 平均計算每個 batch 的損失\n",
    "    )\n",
    "    \n",
    "    # 返回損失值，考慮溫度對梯度的影響\n",
    "    return loss * (temperature ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, teacher_model,criterion, optimizer, epoch,cluster_result,class_centroids):\n",
    "    \n",
    "# def train(train_loader, model, teacher_model,criterion, optimizer, epoch, args, cluster_result):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    total_losses = AverageMeter('TotalLoss', ':.4e')\n",
    "    info_losses = AverageMeter('InfoNCE_Loss', ':.4e')\n",
    "    proto_losses = AverageMeter('ProtoLoss', ':.4e')\n",
    "    centroid_losses = AverageMeter('Centroid Loss', ':.4e')\n",
    "    kd_losses = AverageMeter('KD Loss', ':.4e')\n",
    "\n",
    "    acc_inst = AverageMeter('Acc@Inst', ':6.2f')\n",
    "    acc_proto = AverageMeter('Acc@Proto', ':6.2f')\n",
    "\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, total_losses, info_losses, proto_losses,centroid_losses,kd_losses, acc_inst, acc_proto],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    model.train()\n",
    "    teacher_model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    epoch_iterator = tqdm(train_loader, desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
    "\n",
    "    for i, (images, index) in enumerate(epoch_iterator):\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        images[0] = images[0].cuda(0, non_blocking=True)\n",
    "        images[1] = images[1].cuda(0, non_blocking=True)\n",
    "\n",
    "        output, target,output_proto, student_key,target_proto = model(im_q=images[0], im_k=images[1], cluster_result=cluster_result, index=index)\n",
    "        \n",
    "        print(f\"Output: {output.shape}\")\n",
    "        print(f\"target: {target.shape}\")\n",
    "        \n",
    "        loss_info_value = criterion(output, target)\n",
    "        info_losses.update(loss_info_value.item(), images[0].size(0))\n",
    "        total_loss_value = loss_info_value\n",
    "        \n",
    "        # Knowledge Distillation \n",
    "        # soft targets\n",
    "        # soft label\n",
    "        student_probs = student_key\n",
    "        print(f\"student probs shape:{student_probs.shape}\")\n",
    "        with torch.no_grad():\n",
    "            teacher_embeddings = teacher_model.encoder_q(images[0]).cuda(0)  # Teacher 使用 key encoder\n",
    "            teacher_probs =teacher_embeddings\n",
    "            print(f\"teacher_probs shape:{teacher_probs.shape}\")\n",
    "\n",
    "        kd_loss = knowledge_distillation_loss(teacher_probs, student_probs, temperature=1)\n",
    "        print(f\"kd_loss: {kd_loss}\")\n",
    "        kd_losses.update(kd_loss.item(),images[0].size(0))\n",
    "        \n",
    "        total_loss_value += kd_loss\n",
    "\n",
    "        # # 切換為評估模式，計算質心對齊損失\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            z_q = model.encoder_q(images[0]).cuda(0)  # 明確使用 query 編碼器\n",
    "            z_k = model.encoder_k(images[1]).cuda(0)  # 明確使用 key 編碼器\n",
    "            # # 在此插入檢查代碼\n",
    "            # print(f\"Class centroids device: {class_centroids.device}\")\n",
    "            # print(f\"z_q device: {z_q.device}\")\n",
    "            # print(f\"z_k device: {z_k.device}\")\n",
    "\n",
    "        # # # 確保所有計算都在 GPU\n",
    "        # sim_q = cosine_similarity(z_q.unsqueeze(1), class_centroids.unsqueeze(0), dim=2).cuda(args.gpu)\n",
    "        # sim_k = cosine_similarity(z_k.unsqueeze(1), class_centroids.unsqueeze(0), dim=2).cuda(args.gpu)\n",
    "        sim_q = cosine_similarity_matrix(z_q, class_centroids,eps=1e-8)\n",
    "        sim_k = cosine_similarity_matrix(z_k, class_centroids,eps=1e-8)\n",
    "        print(f\"Sim_q Shape: {sim_q.shape}\")\n",
    "        # print(f\"Sim_q:{sim_q}\\n\")\n",
    "        print(f\"Sim_k Shape: {sim_k.shape}\")\n",
    "        # print(f\"Sim_k:{sim_k}\\n\")\n",
    "\n",
    "        # # 再次檢查 sim_q 和 sim_k\n",
    "        # print(f\"sim_q device: {sim_q.device}\")\n",
    "        # print(f\"sim_k device: {sim_k.device}\")\n",
    "\n",
    "        # # Normalize centroids on GPU\n",
    "        # print(f\"Before normalizing centroids: {class_centroids.device}\")\n",
    "        # class_centroids = F.normalize(class_centroids, p=2, dim=1).cuda(0)\n",
    "        # print(f\"After normalizing centroids: {class_centroids.shape}\")\n",
    "        \n",
    "        # print(f\"z_q device: {z_q.device}, class_centroids device: {class_centroids.device}\")\n",
    "        loss_centroid_value = 1 - F.cosine_similarity(sim_q, sim_k).mean()\n",
    "        # print(f\"F.cosine_similarity(sim_q, sim_k).mean(): {F.cosine_similarity(sim_q, sim_k).mean()}\")\n",
    "        # print(f\"Loss centroid value calculated on device: {loss_centroid_value}\")\n",
    "        model.train()\n",
    "        \n",
    "        # # Update Centroid Loss\n",
    "        total_loss_value += 0.5 * loss_centroid_value\n",
    "        centroid_losses.update(loss_centroid_value.item(), images[0].size(0))\n",
    "        if output_proto is not None:\n",
    "            loss_proto_value = 0\n",
    "            num_cluster = \"1000,1500,2000\"\n",
    "            num_cluster = num_cluster.split(',')\n",
    "            for proto_out, proto_target in zip(output_proto, target_proto):\n",
    "                proto_target = proto_target.cuda(0, non_blocking=True)\n",
    "                proto_loss = criterion(proto_out, proto_target)\n",
    "                loss_proto_value += proto_loss\n",
    "                accp = accuracy(proto_out, proto_target)[0]\n",
    "                acc_proto.update(accp.item(), images[0].size(0))\n",
    "            \n",
    "            if len(num_cluster) > 0:\n",
    "                loss_proto_value /= len(num_cluster)\n",
    "                total_loss_value += loss_proto_value\n",
    "                print(f\"loss_proto_value: {loss_proto_value}, type: {type(loss_proto_value)}\")\n",
    "                proto_losses.update(loss_proto_value.item(), images[0].size(0))\n",
    "\n",
    "\n",
    "        total_losses.update(total_loss_value.item(), images[0].size(0))\n",
    "        acc_result = accuracy(output, target)\n",
    "        # print(f\"Accuracy result: {acc_result}\")  # 應該打印非 None 值\n",
    "        acc = acc_result[0]\n",
    "        acc_inst.update(acc.item(), images[0].size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            progress.display(i)\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"total_loss\": total_losses.avg,\n",
    "        \"info_nce_loss\": info_losses.avg,\n",
    "        \"proto_nce_loss\": proto_losses.avg,\n",
    "        \"centroid_loss\": centroid_losses.avg,\n",
    "        \"kd_loss\":kd_losses.avg,\n",
    "        \"accuracy_inst\": acc_inst.avg,\n",
    "        \"accuracy_proto\": acc_proto.avg,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "828173f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3e9de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/390 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.5141429305076599\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/390 [00:52<5:37:48, 52.10s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  0/390]\tTime 52.106 (52.106)\tData 44.394 (44.394)\tTotalLoss 5.8364e+00 (5.8364e+00)\tInfoNCE_Loss 5.1733e+00 (5.1733e+00)\tProtoLoss 0.0000e+00 (0.0000e+00)\tCentroid Loss 2.9799e-01 (2.9799e-01)\tKD Loss 5.1414e-01 (5.1414e-01)\tAcc@Inst 100.00 (100.00)\tAcc@Proto   0.00 (  0.00)\n",
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4925138056278229\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 2/390 [00:56<2:35:25, 24.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4587862491607666\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 3/390 [01:00<1:35:25, 14.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.431710422039032\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 4/390 [01:05<1:11:08, 11.06s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.40890052914619446\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 5/390 [01:10<56:49,  8.86s/batch]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3954560458660126\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 6/390 [01:15<47:37,  7.44s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.35766375064849854\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 7/390 [01:20<42:37,  6.68s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.35998016595840454\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 8/390 [01:24<38:00,  5.97s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3854227066040039\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 9/390 [01:29<36:01,  5.67s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.44222337007522583\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 10/390 [01:35<35:07,  5.55s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4019940197467804\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 11/390 [01:40<35:12,  5.57s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4421185255050659\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 12/390 [01:45<33:58,  5.39s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.45385879278182983\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 13/390 [01:51<33:44,  5.37s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4639788568019867\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▎         | 14/390 [01:56<33:31,  5.35s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4442075490951538\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 15/390 [02:01<33:15,  5.32s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4400923252105713\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 16/390 [02:05<31:20,  5.03s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4327610731124878\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 17/390 [02:11<31:29,  5.06s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4301336407661438\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▍         | 18/390 [02:16<31:14,  5.04s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.40454956889152527\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▍         | 19/390 [02:21<32:24,  5.24s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4079052209854126\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▌         | 20/390 [02:27<32:54,  5.34s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.37608852982521057\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▌         | 21/390 [02:31<31:19,  5.09s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3703717887401581\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|▌         | 22/390 [02:37<32:22,  5.28s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.36726585030555725\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|▌         | 23/390 [02:42<32:29,  5.31s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.39918723702430725\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|▌         | 24/390 [02:48<33:12,  5.45s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.40758970379829407\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|▋         | 25/390 [02:55<34:50,  5.73s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.39393407106399536\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 26/390 [02:59<32:52,  5.42s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.37990278005599976\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 27/390 [03:05<32:23,  5.35s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.38298311829566956\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 28/390 [03:11<34:23,  5.70s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4255527853965759\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 29/390 [03:16<33:26,  5.56s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3918708562850952\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   8%|▊         | 30/390 [03:22<33:34,  5.59s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3990691304206848\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   8%|▊         | 31/390 [03:27<33:23,  5.58s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3797306418418884\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   8%|▊         | 32/390 [03:33<32:54,  5.52s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.38976791501045227\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   8%|▊         | 33/390 [03:38<33:02,  5.55s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3813732862472534\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   9%|▊         | 34/390 [03:44<33:44,  5.69s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4056026339530945\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   9%|▉         | 35/390 [03:49<32:10,  5.44s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.37723106145858765\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   9%|▉         | 36/390 [03:55<31:47,  5.39s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3858591914176941\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   9%|▉         | 37/390 [04:01<32:41,  5.56s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.37976494431495667\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  10%|▉         | 38/390 [04:07<33:33,  5.72s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.4020177125930786\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  10%|█         | 39/390 [04:13<33:41,  5.76s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3792133927345276\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  10%|█         | 40/390 [04:18<33:03,  5.67s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3900160789489746\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  11%|█         | 41/390 [04:23<31:33,  5.43s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3946141004562378\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  11%|█         | 42/390 [04:28<30:47,  5.31s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.36189591884613037\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  11%|█         | 43/390 [04:32<29:21,  5.08s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3862631916999817\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  11%|█▏        | 44/390 [04:37<28:32,  4.95s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([256, 513])\n",
      "target: torch.Size([256])\n",
      "student probs shape:torch.Size([256, 128])\n",
      "teacher_probs shape:torch.Size([256, 128])\n",
      "kd_loss: 0.3800046443939209\n",
      "Sim_q Shape: torch.Size([256, 200])\n",
      "Sim_k Shape: torch.Size([256, 200])\n",
      "After normalizing centroids: torch.Size([200, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  12%|█▏        | 45/390 [04:47<36:44,  6.39s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     cluster_result \u001b[38;5;241m=\u001b[39m run_kmeans(features)\n\u001b[0;32m      8\u001b[0m adjust_learning_rate(optimizer, epoch)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclass_centroids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# train(train_loader, model,teacher_model, criterion, optimizer, epoch, args, cluster_result)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, teacher_model, criterion, optimizer, epoch, cluster_result, class_centroids)\u001b[0m\n\u001b[0;32m     29\u001b[0m images[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda(\u001b[38;5;241m0\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m images[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda(\u001b[38;5;241m0\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 32\u001b[0m output, target,output_proto, target_proto \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\k3866\\Documents\\Projects\\Dual_Centroid_Contrastive_Knowledge_Distillation\\pcl\\builder.py:136\u001b[0m, in \u001b[0;36mMoCo.forward\u001b[1;34m(self, im_q, im_k, is_eval, cluster_result, index)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# no gradient to keys\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_momentum_update_key_encoder()  \u001b[38;5;66;03m# update the key encoder\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_k\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# keys: NxC\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     k \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# compute query features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\models\\resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\models\\resnet.py:147\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    144\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    146\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m--> 147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\k3866\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:173\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[0;32m    175\u001b[0m             exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0,200):\n",
    "    cluster_result = None\n",
    "    if epoch >= 1:\n",
    "        # cluster_result\n",
    "        features = compute_features(eval_loader, model)\n",
    "        cluster_result = run_kmeans(features)\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(train_loader, model,teacher_model, criterion, optimizer, epoch, cluster_result,class_centroids)\n",
    "    # train(train_loader, model,teacher_model, criterion, optimizer, epoch, args, cluster_result)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch':'resnet',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best=False, filename='{}/checkpoint_{:04d}.pth.tar'.format('experiment_pcl', epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
